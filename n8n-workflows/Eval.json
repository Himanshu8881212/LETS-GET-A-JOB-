{
  "name": "Eval",
  "nodes": [
    {
      "parameters": {
        "promptType": "define",
        "text": "=Resume : {{ $json.body.resume_text }}\n\nCover Letter (CL) : {{ $json.body.cover_letter_text }}\n\nJob Description (JD) : {{ $json.body.job_description }}",
        "hasOutputParser": true,
        "options": {
          "systemMessage": "ROLE\nYou are a strict ATS-style evaluator. Given a job description (JD) plus a candidate‚Äôs resume and cover letter (CL), your job is to grade fit on five metrics, be brutally candid, and provide concrete, actionable fixes. Do not invent experiences or facts‚Äîbase everything on the supplied texts only.\n\nINPUT\n- job_description: plain text\n- resume_text: plain text\n- cover_letter_text: plain text (optional)\n\nGENERAL RULES\n- Be nitpicky and specific. Quote short snippets (‚â§25 words) from the resume/CL/JD to justify claims.\n- No hallucinations. If a requirement/skill isn‚Äôt present in the resume/CL, mark it ‚ÄúNot found‚Äù and suggest how to add it truthfully (e.g., quantify an existing bullet, clarify scope, name a tool already implied).\n- Normalize keywords (singular/plural/stems; e.g., ‚Äúmodeling‚Äù ~ ‚Äúmodels‚Äù), and accept close synonyms only when obvious (e.g., ‚ÄúPower BI‚Äù vs ‚ÄúBI dashboards‚Äù is NOT equivalent‚Äîcall the gap out).\n- Penalize vague language (‚Äúhelped,‚Äù ‚Äúworked on,‚Äù ‚Äúresponsible for‚Äù) and unquantified claims.\n- If the CL is generic or repeats the resume, say so and explain how to tailor it to the JD.\n\nTHE FIVE METRICS (score each 0‚Äì10)\n1) Role Fit & Requirements Match (weight 30%)\n   - Coverage of must-have qualifications, responsibilities, domain/industry alignment, years/level.\n2) Skills & Keywords Coverage (weight 25%)\n   - Presence of exact tools/frameworks/methods named in the JD; density and placement in resume and CL; recency.\n3) Impact & Evidence Quality (weight 20%)\n   - Quantified outcomes, scope, complexity, concrete achievements (STAR); weak/verby phrasing penalized.\n4) ATS Compliance & Clarity (weight 15%)\n   - Parseability (single column, standard headings), contact info present, file-unsafe elements (tables, images, icons), clean formatting, consistent dates.\n5) Cover Letter Effectiveness (weight 10%)\n   - Tailoring to JD, value proposition, concise narrative, company motivation, concrete proof points tied to JD.\n\nSCORING ANCHORS (apply to each metric)\n- 0‚Äì2 Poor: Major gaps; critical must-haves missing; generic or unreadable.\n- 3‚Äì4 Weak: Several must-haves missing; weak evidence; many formatting issues.\n- 5‚Äì6 Fair: Some match; uneven keywords; limited impact proof; CL under-tailored.\n- 7‚Äì8 Strong: Most requirements matched; good keyword coverage; clear impact; minor issues.\n- 9‚Äì10 Exceptional: Fully matched with strong, recent, quantified evidence; tailored CL; ATS-perfect.\n\nOUTPUT FORMAT (JSON only; no extra prose)\n{\n  \"summary\": \"Brief overview of fit and biggest blockers.\",\n  \"overall\": {\n    \"weighted_score\": 0,\n    \"weights\": {\n      \"role_fit\": 0.30,\n      \"skills_keywords\": 0.25,\n      \"impact_evidence\": 0.20,\n      \"ats_compliance\": 0.15,\n      \"cover_letter\": 0.10\n    },\n    \"rationale\": \"One-liner explaining the overall score.\"\n  },\n  \"scores\": {\n    \"role_fit\": {\n      \"score\": 0,\n      \"evidence_found\": [\"Quoted line from resume/CL\", \"Quoted line from JD\"],\n      \"gaps\": [\"Missing must-have X\", \"No domain experience Y\"],\n      \"fixes\": [\"Add project showing X with metrics\", \"Clarify years doing Y\"]\n    },\n    \"skills_keywords\": {\n      \"score\": 0,\n      \"present\": [\"Exact term A\", \"Exact term B\"],\n      \"missing\": [\"Exact term C\", \"Exact term D\"],\n      \"fixes\": [\"Add C in Skills if true\", \"Name tool D in Project Z bullet\"]\n    },\n    \"impact_evidence\": {\n      \"score\": 0,\n      \"issues\": [\"Vague verbs\", \"No metrics\", \"Unclear scope\"],\n      \"example_rewrites\": [\n        \"Replace: \\\"Worked on reporting\\\" -> \\\"Built Power BI suite cutting refresh time 6h->45m (-87%)\\\"\"\n      ]\n    },\n    \"ats_compliance\": {\n      \"score\": 0,\n      \"problems\": [\"Two-column layout\", \"Icons in headers\", \"Missing dates\"],\n      \"fixes\": [\"Use single-column\", \"Standard section names\", \"Add MM/YYYY ranges\"]\n    },\n    \"cover_letter\": {\n      \"score\": 0,\n      \"critique\": [\"Generic opener\", \"Repeats resume\", \"No tie to company\"],\n      \"fixes\": [\"Open with JD-linked value\", \"Cite 2 quantified wins\", \"Keep ‚â§300 words\"]\n    }\n  },\n  \"missing_keywords\": {\n    \"must_have\": [\"Term from JD essential 1\", \"Term essential 2\"],\n    \"should_have\": [\"Important term 1\"],\n    \"nice_to_have\": [\"Optional term 1\"]\n  },\n  \"top_fixes\": [\n    {\n      \"area\": \"Resume ‚Üí Experience\",\n      \"action\": \"Quantify outcomes for Project A\",\n      \"why\": \"JD requires measurable impact\",\n      \"example\": \"Automated pipeline reducing failures 28% QoQ\"\n    },\n    {\n      \"area\": \"Resume ‚Üí Skills\",\n      \"action\": \"List exact JD tools actually used\",\n      \"why\": \"Improves ATS matching\",\n      \"example\": \"Skills: Snowflake, dbt, Power BI, Celonis, Python\"\n    },\n    {\n      \"area\": \"Cover Letter\",\n      \"action\": \"Tie achievements to JD responsibilities\",\n      \"why\": \"Signals tailoring\",\n      \"example\": \"Shipped prompt-eval harness raising success 62%‚Üí79%\"\n    }\n  ],\n  \"keyword_occurrences\": [\n    {\n      \"term\": \"Power BI\",\n      \"in_resume\": 2,\n      \"in_cover_letter\": 0,\n      \"in_job_description\": 5\n    }\n  ],\n  \"risk_flags\": [\"Employment gaps\", \"Tech mismatch (Tableau vs Power BI)\", \"Language mismatch\"],\n  \"source\": {\n    \"job_description_id\": \"jd_123\",\n    \"resume_id\": \"cv_456\",\n    \"cover_letter_id\": \"cl_789\"\n  }\n}\n\n\nEVALUATION PROCEDURE\n1) From the JD, list must-have qualifications, responsibilities, and named tools. Separate MUST vs NICE-TO-HAVE.\n2) Scan resume/CL for exact matches; then close synonyms/stems (be conservative). Count occurrences and note recency (latest roles first).\n3) Identify missing or weakly evidenced items. Penalize vague bullets and unquantified claims.\n4) Check ATS basics: single-column, standard section headings, contact info, consistent date formats, no images/icons/text boxes.\n5) Assess CL tailoring: opening hook tied to role/company, concrete evidence mapped to JD, concise close with call to action.\n6) Assign each metric 0‚Äì10 using anchors; compute weighted overall score. Provide pointed, high-impact fixes and example rewrites **only using existing facts** (if data is missing, instruct what to add‚Äînot fabricate).\n\nCONSTRAINTS\n- JSON output only, exactly as specified.\n- Quote just enough text to justify (‚â§25 words per quote).\n- If cover_letter_text is missing, still score the CL metric but explain the penalty.\n"
        }
      },
      "type": "@n8n/n8n-nodes-langchain.agent",
      "typeVersion": 2.2,
      "position": [
        144,
        0
      ],
      "id": "a1c5eff0-d77b-4cbb-83e6-8335a9dbe259",
      "name": "AI Agent",
      "executeOnce": false,
      "retryOnFail": true,
      "maxTries": 5
    },
    {
      "parameters": {
        "model": "moonshotai/kimi-k2-instruct-0905",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatGroq",
      "typeVersion": 1,
      "position": [
        112,
        224
      ],
      "id": "31adad02-2a5a-4ffe-8e92-93de42651671",
      "name": "Groq Chat Model",
      "credentials": {
        "groqApi": {
          "id": "7O9GJEranDwcy24L",
          "name": "Groq account"
        }
      }
    },
    {
      "parameters": {
        "jsonSchemaExample": "{\n  \"summary\": \"Brief overview of fit and biggest blockers.\",\n  \"overall\": {\n    \"weighted_score\": 0,\n    \"weights\": {\n      \"role_fit\": 0.30,\n      \"skills_keywords\": 0.25,\n      \"impact_evidence\": 0.20,\n      \"ats_compliance\": 0.15,\n      \"cover_letter\": 0.10\n    },\n    \"rationale\": \"One-liner explaining the overall score.\"\n  },\n  \"scores\": {\n    \"role_fit\": {\n      \"score\": 0,\n      \"evidence_found\": [\"Quoted line from resume/CL\", \"Quoted line from JD\"],\n      \"gaps\": [\"Missing must-have X\", \"No domain experience Y\"],\n      \"fixes\": [\"Add project showing X with metrics\", \"Clarify years doing Y\"]\n    },\n    \"skills_keywords\": {\n      \"score\": 0,\n      \"present\": [\"Exact term A\", \"Exact term B\"],\n      \"missing\": [\"Exact term C\", \"Exact term D\"],\n      \"fixes\": [\"Add C in Skills if true\", \"Name tool D in Project Z bullet\"]\n    },\n    \"impact_evidence\": {\n      \"score\": 0,\n      \"issues\": [\"Vague verbs\", \"No metrics\", \"Unclear scope\"],\n      \"example_rewrites\": [\n        \"Replace: \\\"Worked on reporting\\\" -> \\\"Built Power BI suite cutting refresh time 6h->45m (-87%)\\\"\"\n      ]\n    },\n    \"ats_compliance\": {\n      \"score\": 0,\n      \"problems\": [\"Two-column layout\", \"Icons in headers\", \"Missing dates\"],\n      \"fixes\": [\"Use single-column\", \"Standard section names\", \"Add MM/YYYY ranges\"]\n    },\n    \"cover_letter\": {\n      \"score\": 0,\n      \"critique\": [\"Generic opener\", \"Repeats resume\", \"No tie to company\"],\n      \"fixes\": [\"Open with JD-linked value\", \"Cite 2 quantified wins\", \"Keep ‚â§300 words\"]\n    }\n  },\n  \"missing_keywords\": {\n    \"must_have\": [\"Term from JD essential 1\", \"Term essential 2\"],\n    \"should_have\": [\"Important term 1\"],\n    \"nice_to_have\": [\"Optional term 1\"]\n  },\n  \"top_fixes\": [\n    {\n      \"area\": \"Resume ‚Üí Experience\",\n      \"action\": \"Quantify outcomes for Project A\",\n      \"why\": \"JD requires measurable impact\",\n      \"example\": \"Automated pipeline reducing failures 28% QoQ\"\n    },\n    {\n      \"area\": \"Resume ‚Üí Skills\",\n      \"action\": \"List exact JD tools actually used\",\n      \"why\": \"Improves ATS matching\",\n      \"example\": \"Skills: Snowflake, dbt, Power BI, Celonis, Python\"\n    },\n    {\n      \"area\": \"Cover Letter\",\n      \"action\": \"Tie achievements to JD responsibilities\",\n      \"why\": \"Signals tailoring\",\n      \"example\": \"Shipped prompt-eval harness raising success 62%‚Üí79%\"\n    }\n  ],\n  \"keyword_occurrences\": [\n    {\n      \"term\": \"Power BI\",\n      \"in_resume\": 2,\n      \"in_cover_letter\": 0,\n      \"in_job_description\": 5\n    }\n  ],\n  \"risk_flags\": [\"Employment gaps\", \"Tech mismatch (Tableau vs Power BI)\", \"Language mismatch\"],\n  \"source\": {\n    \"job_description_id\": \"jd_123\",\n    \"resume_id\": \"cv_456\",\n    \"cover_letter_id\": \"cl_789\"\n  }\n}\n"
      },
      "type": "@n8n/n8n-nodes-langchain.outputParserStructured",
      "typeVersion": 1.3,
      "position": [
        288,
        224
      ],
      "id": "4de7d23c-6832-45f0-9cc2-85aa3847c889",
      "name": "Structured Output Parser"
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $json }}",
        "options": {}
      },
      "id": "9ab36a5b-d1a0-4965-a764-fd73d4c29a4d",
      "name": "Respond to Webhook",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.1,
      "position": [
        608,
        0
      ]
    },
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "evaluate-ats",
        "responseMode": "responseNode",
        "options": {}
      },
      "id": "a5f3dacb-e5d5-4af8-8f86-5b788cfb9f34",
      "name": "Webhook",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2.1,
      "position": [
        -112,
        0
      ],
      "webhookId": "0f93e18c-1bd2-4b15-8401-841c4ab4ed83"
    }
  ],
  "pinData": {
    "Webhook": [
      {
        "json": {
          "headers": {
            "host": "atomic-dayton-squad-printable.trycloudflare.com",
            "user-agent": "node",
            "content-length": "12617",
            "accept": "*/*",
            "accept-encoding": "gzip",
            "accept-language": "*",
            "cdn-loop": "cloudflare; loops=1; subreqs=1",
            "cf-connecting-ip": "2001:a61:5e8:b201:89b:67bd:e0a5:e487",
            "cf-ew-via": "15",
            "cf-ipcountry": "DE",
            "cf-ray": "9997d10631239276-MUC",
            "cf-visitor": "{\"scheme\":\"https\"}",
            "cf-warp-tag-id": "8fe37b18-359e-4dde-ad62-f896566a654b",
            "cf-worker": "trycloudflare.com",
            "connection": "keep-alive",
            "content-type": "application/json",
            "sec-fetch-mode": "cors",
            "x-forwarded-for": "2001:a61:5e8:b201:89b:67bd:e0a5:e487",
            "x-forwarded-proto": "https"
          },
          "params": {},
          "query": {},
          "body": {
            "resume_text": "Himanshu Ninawe  \nüìû +49-17660379682, üìå Munich, Germany, üìß Ninawe.himanshu.24@gmail.com,‚ö° Himanshu Ninawe  \n\nWork Experience  \n\nData Engineer & Operation Analyst, at Knorr-Bremse SfS GmbH, Munich, Germany. (Jan 24 - Present)  \n‚óè Data Integration & Visualization: Performed ETL on data from Snowflake and SAP BW, creating insightful Power BI dashboards for operations KPIs, which enhanced decision-making processes.  \n‚óè Advanced Analytics: Conducted descriptive, root cause, sensitivity, impact, and predictive analyses to improve process efficiency and operational performance.  \n‚óè Machine Learning Development: Developed predictive models using Python, enabling initiative-taking strategies, and improving forecasting accuracy.  \n‚óè Process Mining Optimization: Applied process mining techniques with Celonis to streamline operational workflows, reducing inefficiencies and increasing throughput.  \n‚óè Global Business Process Leadership: As the Global BPR for operations across all Rail plants, led monthly KPI reviews, driving performance improvements and contributing to the achievement of yearly targets.  \n\nMaster‚Äôs Thesis Student, at Ravensburg-Weingarten University, Weingarten, Germany. (March 24 ‚Äì August 24)  \n‚ÄúIntegration of Generative AI in Smart Assistants‚Äù  \n‚óè Open-Source Smart Assistant Development: Created an open-source home assistant with Generative AI capabilities to overcome limitations like natural, human‚Äëlike conversations, ground‚Äëup multilingual support, and offline functionality for non‚Äëreal‚Äëtime information.  \n‚óè Comprehensive LLM Experimentation: Conducted extensive experiments with various methods including supervised fine‚Äëtuning with Q‚ÄëLoRa, LLM function calling, Retrieval Augmented Generation (RAG) and Agentic workflows.  \n‚óè Custom Dataset Generation: Generated a high‚Äëquality Alpaca‚Äëformat dataset of approximately 1,000 entries using Llama 3 70B and Nematron 405B reward models; manually reviewed responses to enhance system accuracy.  \n‚óè Edge Device Integration & Communication: Enabled task execution on an ESP32 connected to devices like lights and switches; established MQTT communication between the Raspberry Pi and ESP32, allowing offline operation without internet dependency.  \n‚óè System performance Enhancement: Achieved a 95% accuracy, 90% precision, and 92% flexibility. Validated these results on a custom validation dataset featuring multilingual prompts, different moods, tones, and grammatical structures.  \n\nWorking Student, for MTU Solutions GmbH, Weingarten, Germany. (May 23 - October 23)  \n‚óè Data Generation: Generated and processed 1.5‚ÄØmillion rows of synthetic data from a digital twin, leveraging automated scripts to ensure high‚Äëvolume, diverse datasets.  \n‚óè Data Cleaning & Feature Engineering: Conducted data cleaning and feature engineering, increasing dataset variability by 20% through advanced techniques like Latin hypercube sampling.  \n‚óè Predictive Modeling: Developed and optimized machine learning models (Random Forest, MLP, XGBoost, LSTM), improving fault localization accuracy, with ensemble methods performing best.  \n‚óè Sensitivity Analysis & Feature Importance: Performed global and local sensitivity analyses and feature importance evaluation using methods like mRMR to identify key parameters influencing engine performance.  \n‚óè Performance Improvement: Enhanced fault detection and reduced root cause analysis time, improving overall engine maintenance efficiency and downtime reduction.  \n\nProjects  \n\nMultimodal Information Retrieval System  \n‚óè Multimodal Embeddings: For context involving images, text, and videos, used Nomic vision embedding models for visual data and Nomic text embedding models for textual data and ChromaDB to store the embeddings.  \n‚óè Vision‚ÄëLanguage Model Integration: Integrated Moondream‚ÄØ2, a Phi2‚Äëbased Vision Language Model (VLM), to enhance the processing and understanding of multimodal data.  \n‚óè Enhanced Retrieval Capabilities: Improved retrieval and generation capabilities by combining multiple modalities, leading to more comprehensive and accurate outputs.  \n\nDigital Twin Fault Diagnostics Using LLMs  \n‚óè Synthetic Data Generation and Transformation: Generated synthetic data from a digital twin of a single‚Äëcylinder engine, performed EDA, cleaned the data, and transformed it into a DPO dataset.  \n‚óè Model Fine‚ÄëTuning: Fine‚Äëtuned the Phi‚ÄØ2 model to function as a digital twin, inputting real‚Äëtime data from six sensors and outputting 132 parameter sensor values to identify fault locations.  \n‚óè LLM‚ÄëBased Fault Diagnostics: Utilized the fine‚Äëtuned LLM as a predictive model for fault diagnostics, enhancing the ability to detect and locate engine faults effectively.  \n\nImplementing Long‚ÄëTerm Memory in Language Models  \n‚óè Conditional RAG Implementation: Employed Conditional Retrieval Augmented Generation to enable long‚Äëterm memory in LLMs by storing entire conversations in text files, which are recalled when a prompt ends with double question marks \"??\".  \n‚óè Enhanced Recall with Graph RAG: Used Graph RAG to improve recall accuracy and precision, allowing the model to retrieve relevant past conversation data more effectively.  \n‚óè Mixture of Experts (MoE) Model Integration: Integrated Phixtral, a Mixture of Experts model adept at \"needle in a haystack\" tasks, to enhance the model's performance in recalling specific information.  \n\nEducation  \n\n‚óè MSc in Mechatronics (MINT), Ravensburg‚ÄëWeingarten University, Germany. GPA (2.1/5.0)  \n‚óè B.E in Mechanical Engineering, Rashtrasant Tukadoji Maharaj Nagpur University, India. GPA (2.1/5.0)  \n\nSpecializations  \n\n‚óè IBM Machine Learning Professional Certificate.  \n‚óè IBM AI Engineering Professional Certificate.  \n\nSkills  \n\n‚óè Programming & Scripting: Python, Linux, SQL, Prompting.  \n‚óè Data Analysis and Visualization: Snowflake, NumPy and Pandas, Power BI, Excel, Qlik sense, SAP, Celonis.  \n‚óè ML Frameworks: TensorFlow, PyTorch, Scikit‚Äëlearn, XGBoost, Lang‚Äëchain, Ollama, Crew AI, Autogen, Graph RAG, Transformers, Diffusers.  \n‚óè Orchestration and Deployment: GitHub, MLflow, Docker, Rest API.  \n‚óè Database Management: RDBMS, NoSQL, Vector DB  \n‚óè Technical Skills: Data Wrangling, Data Modeling, Data Visualization, Process Mining, Machine Learning, Fine‚Äëtuning, Agentic Workflows, Synthetic Data generation.  \n‚óè Soft Skills: Critical Thinking, Problem‚ÄëSolving, Excellent Communication, Team Collaboration, Leadership, Business Acumen.  \n\nLanguages  \n\nEnglish (C1), German (B1)",
            "cover_letter_text": "Himanshu Ninawe\nWildenfelser Stra√üe 15\n81249 M√ºnchen, Germany\n\n22 October 2025\n\nHiring Manager\nSynthflow AI\nBerlin, Germany\n\nDear Hiring Manager,\n\nI am applying for the Applied LLM Engineer role at Synthflow AI, which I discovered on your careers page.\n\nI build production-grade LLM systems‚Äîprompted, retrieval-aware, and observable‚Äîusing clean Python and rigorous evaluation so improvements reach users, not just notebooks.\n\nSynthflow‚Äôs mission and scale resonate: 45M+ calls handled with 99.9% uptime, 5M+ hours saved, and 1,000+ customers. Your no-code platform and tight customer feedback loops match my focus on shipping dependable agents that measurably raise task completion and first-pass accuracy.\n\nRelevance of my skills: At Knorr-Bremse (Munich), I ship Python services and analytics to production (Snowflake/SAP BW ‚Üí KPI platforms) and lead data/ML work informing monthly global reviews. In my M.Sc. thesis, I fine-tuned LLMs with parameter-efficient methods and built an evaluation harness, achieving 95% accuracy and 90% precision on a custom multilingual set. My projects include RAG pipelines (ChromaDB, graph-style retrieval, reranking) and agentic workflows, plus ML lifecycle tooling (Git, MLflow, Docker). Earlier at MTU Solutions, I generated/engineered 1.5M synthetic rows, boosting dataset variability by ~20% and improving fault-localization performance.\n\nHow I‚Äôll add value: I can design and iterate system/tool prompts tuned to call outcomes (intent/slot extraction, refusal policy, redaction), build a prompt-authoring co-pilot that lints risks, proposes tool schemas, and auto-generates eval cases from conversation logs, stand up config-as-code with versioned prompt bundles, rollout/rollback, and guardrails (pydantic validation, retrieval-aware prompting, safe fallbacks), and create offline/online evals (rubrics, regressions, canaries/A-B) wired into dashboards tracking accuracy, latency, cost, and robustness. I‚Äôm disciplined about data hygiene, PII handling, and regional boundaries.\n\nIn sum, I bring hands-on prompt/RAG work, a production mindset, and an evaluation-first approach aligned with Synthflow‚Äôs applied focus. I‚Äôd welcome the opportunity to discuss how I can contribute to your customer co-pilot and the reasoning layer at scale.\n\nThank you for your time and consideration.\n\nSincerely,\nHimanshu Ninawe",
            "job_description": "Job Title: Applied LLM Engineer - Prompts, Evals & Agents\nCompany: G2i Inc.\nCompany Website: Not stated.\nLocation: Global Remote\nWork Model: Remote\nSeniority: Not stated.\nEmployment Type: Full time\nDepartment/Team: Engineering\nPosted: Not stated.\nApply by: Not stated.\nCompensation: Not stated.\nVisa/Relocation/Travel: Not stated.\n\nSummary:\nSynthflow AI is a no-code platform for deploying voice AI agents that automate phone calls across contact center operations and business process outsourcing (BPO) at scale. We help mid-market and enterprise companies manage routine calls to save teams time and resources.\n\nRequirements:\n- Python: 3+ years writing clean, tested, production code (typing, pytest, profiling); experience building small services/APIs (FastAPI preferred).\n- Prompt Engineering: Hands-on experience designing system/tool prompts, meta-prompting, rubric graders, and iterative prompt tuning based on real user data.\n- LLM Integration: Comfortable with major APIs (OpenAI/Anthropic/Google/Mistral), function/tool calling, streaming, and robust output handling.\n- Evaluation Mindset: Ability to define measurable success, create labeled datasets, and run methodical experiments/A/B tests.\n- Product Sense: Comfortable talking with customers, turning qualitative feedback into shipped improvements.\n- Data Hygiene: Practical experience cleaning, labeling, and balancing datasets; awareness of privacy/PII constraints.\n\nResponsibilities:\n- Design & iterate prompts (system, tool/function-calling, task prompts) to boost voice AI agent success, reliability, and tone.\n- Build co-pilots for customers to author their own prompts: meta-prompted assistants that suggest structures, lint for risks, autocomplete tool schemas, critique drafts, and generate eval cases.\n- Work directly with customer feedback and conversation logs to identify failure modes; translate them into prompt changes, guardrails, and data improvements.\n- Build eval datasets (success labels, rubrics, edge cases, regressions) and run offline/online evaluations (A/B tests, canaries) to quantify impact.\n- Create Python utilities/services for prompt versioning, config-as-code, rollout/rollback, and guardrails (policies, refusals, redaction).\n- Partner with PM/Success to define success metrics (task completion, first-pass accuracy, cost, latency) and instrument dashboards/alerts.\n- Own LLM integration details: function/tool schemas, output parsing/validation (pydantic), retrieval-aware prompting, and fallback strategies.\n- Ensure privacy & compliance (PII handling, anonymization, regional data boundaries) in datasets and logs.\n- Share learnings via concise docs, playbooks, and internal demos.\n\nTech Stack:\n- Python\n- FastAPI\n- OpenAI\n- Anthropic\n- Google\n- Mistral\n- pydantic\n- DSpy\n- MCP\n- LangGraph\n- LlamaIndex\n- Rasa\n- Langfuse\n- LangSmith\n- Braintrust\n- Qdrant\n- Weaviate\n- Pinecone\n- SQL\n- Grafana\n- OTel\n\nBenefits:\n- Own the reasoning layer and the customer co-pilot experience used at scale.\n- Ship fast in a tight customer feedback loop and see your impact measured in days, not quarters.\n\nApplication:\n- How to apply: Not stated.\n- Apply link/email: https://jobs.ashbyhq.com/synthflow/d5bba02f-1708-4368-aba1-1afdf695af40?utm_source=3bVopaRB11\n\nReference/Job ID: d5bba02f-1708-4368-aba1-1afdf695af40\n\nNotes:\nNot stated.\n\nSource: https://jobs.ashbyhq.com/synthflow/d5bba02f-1708-4368-aba1-1afdf695af40?utm_source=3bVopaRB11"
          },
          "webhookUrl": "https://atomic-dayton-squad-printable.trycloudflare.com/webhook/evaluate-ats",
          "executionMode": "production"
        }
      }
    ]
  },
  "connections": {
    "Groq Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "AI Agent",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Structured Output Parser": {
      "ai_outputParser": [
        [
          {
            "node": "AI Agent",
            "type": "ai_outputParser",
            "index": 0
          }
        ]
      ]
    },
    "AI Agent": {
      "main": [
        [
          {
            "node": "Respond to Webhook",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Webhook": {
      "main": [
        [
          {
            "node": "AI Agent",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "1b2931d7-8ca1-48ab-9a45-64eec904f9ae",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "91619af48a111996d57e467ff2315ebcea6bea0614914ccbffc4272c1511d582"
  },
  "id": "pyGw8Xkfa9DkwfLI",
  "tags": []
}