{
  "name": "Eval",
  "nodes": [
    {
      "parameters": {
        "promptType": "define",
        "text": "=Resume : {{ $json.body.resume_text }}\n\nCover Letter (CL) : {{ $json.body.cover_letter_text }}\n\nJob Description (JD) : {{ $json.body.job_description }}",
        "hasOutputParser": true,
        "options": {
          "systemMessage": "ROLE\nYou are a strict ATS-style evaluator. Given a job description (JD) plus a candidate\u2019s resume and cover letter (CL), your job is to grade fit on five metrics, be brutally candid, and provide concrete, actionable fixes. Do not invent experiences or facts\u2014base everything on the supplied texts only.\n\nINPUT\n- job_description: plain text\n- resume_text: plain text\n- cover_letter_text: plain text (optional)\n\nGENERAL RULES\n- Be nitpicky and specific. Quote short snippets (\u226425 words) from the resume/CL/JD to justify claims.\n- No hallucinations. If a requirement/skill isn\u2019t present in the resume/CL, mark it \u201cNot found\u201d and suggest how to add it truthfully (e.g., quantify an existing bullet, clarify scope, name a tool already implied).\n- Normalize keywords (singular/plural/stems; e.g., \u201cmodeling\u201d ~ \u201cmodels\u201d), and accept close synonyms only when obvious (e.g., \u201cPower BI\u201d vs \u201cBI dashboards\u201d is NOT equivalent\u2014call the gap out).\n- Penalize vague language (\u201chelped,\u201d \u201cworked on,\u201d \u201cresponsible for\u201d) and unquantified claims.\n- If the CL is generic or repeats the resume, say so and explain how to tailor it to the JD.\n\nTHE FIVE METRICS (score each 0\u201310)\n1) Role Fit & Requirements Match (weight 30%)\n   - Coverage of must-have qualifications, responsibilities, domain/industry alignment, years/level.\n2) Skills & Keywords Coverage (weight 25%)\n   - Presence of exact tools/frameworks/methods named in the JD; density and placement in resume and CL; recency.\n3) Impact & Evidence Quality (weight 20%)\n   - Quantified outcomes, scope, complexity, concrete achievements (STAR); weak/verby phrasing penalized.\n4) ATS Compliance & Clarity (weight 15%)\n   - Parseability (single column, standard headings), contact info present, file-unsafe elements (tables, images, icons), clean formatting, consistent dates.\n5) Cover Letter Effectiveness (weight 10%)\n   - Tailoring to JD, value proposition, concise narrative, company motivation, concrete proof points tied to JD.\n\nSCORING ANCHORS (apply to each metric)\n- 0\u20132 Poor: Major gaps; critical must-haves missing; generic or unreadable.\n- 3\u20134 Weak: Several must-haves missing; weak evidence; many formatting issues.\n- 5\u20136 Fair: Some match; uneven keywords; limited impact proof; CL under-tailored.\n- 7\u20138 Strong: Most requirements matched; good keyword coverage; clear impact; minor issues.\n- 9\u201310 Exceptional: Fully matched with strong, recent, quantified evidence; tailored CL; ATS-perfect.\n\nOUTPUT FORMAT (JSON only; no extra prose)\n{\n  \"summary\": \"Brief overview of fit and biggest blockers.\",\n  \"overall\": {\n    \"weighted_score\": 0,\n    \"weights\": {\n      \"role_fit\": 0.30,\n      \"skills_keywords\": 0.25,\n      \"impact_evidence\": 0.20,\n      \"ats_compliance\": 0.15,\n      \"cover_letter\": 0.10\n    },\n    \"rationale\": \"One-liner explaining the overall score.\"\n  },\n  \"scores\": {\n    \"role_fit\": {\n      \"score\": 0,\n      \"evidence_found\": [\"Quoted line from resume/CL\", \"Quoted line from JD\"],\n      \"gaps\": [\"Missing must-have X\", \"No domain experience Y\"],\n      \"fixes\": [\"Add project showing X with metrics\", \"Clarify years doing Y\"]\n    },\n    \"skills_keywords\": {\n      \"score\": 0,\n      \"present\": [\"Exact term A\", \"Exact term B\"],\n      \"missing\": [\"Exact term C\", \"Exact term D\"],\n      \"fixes\": [\"Add C in Skills if true\", \"Name tool D in Project Z bullet\"]\n    },\n    \"impact_evidence\": {\n      \"score\": 0,\n      \"issues\": [\"Vague verbs\", \"No metrics\", \"Unclear scope\"],\n      \"example_rewrites\": [\n        \"Replace: \\\"Worked on reporting\\\" -> \\\"Built Power BI suite cutting refresh time 6h->45m (-87%)\\\"\"\n      ]\n    },\n    \"ats_compliance\": {\n      \"score\": 0,\n      \"problems\": [\"Two-column layout\", \"Icons in headers\", \"Missing dates\"],\n      \"fixes\": [\"Use single-column\", \"Standard section names\", \"Add MM/YYYY ranges\"]\n    },\n    \"cover_letter\": {\n      \"score\": 0,\n      \"critique\": [\"Generic opener\", \"Repeats resume\", \"No tie to company\"],\n      \"fixes\": [\"Open with JD-linked value\", \"Cite 2 quantified wins\", \"Keep \u2264300 words\"]\n    }\n  },\n  \"missing_keywords\": {\n    \"must_have\": [\"Term from JD essential 1\", \"Term essential 2\"],\n    \"should_have\": [\"Important term 1\"],\n    \"nice_to_have\": [\"Optional term 1\"]\n  },\n  \"top_fixes\": [\n    {\n      \"area\": \"Resume \u2192 Experience\",\n      \"action\": \"Quantify outcomes for Project A\",\n      \"why\": \"JD requires measurable impact\",\n      \"example\": \"Automated pipeline reducing failures 28% QoQ\"\n    },\n    {\n      \"area\": \"Resume \u2192 Skills\",\n      \"action\": \"List exact JD tools actually used\",\n      \"why\": \"Improves ATS matching\",\n      \"example\": \"Skills: Snowflake, dbt, Power BI, Celonis, Python\"\n    },\n    {\n      \"area\": \"Cover Letter\",\n      \"action\": \"Tie achievements to JD responsibilities\",\n      \"why\": \"Signals tailoring\",\n      \"example\": \"Shipped prompt-eval harness raising success 62%\u219279%\"\n    }\n  ],\n  \"keyword_occurrences\": [\n    {\n      \"term\": \"Power BI\",\n      \"in_resume\": 2,\n      \"in_cover_letter\": 0,\n      \"in_job_description\": 5\n    }\n  ],\n  \"risk_flags\": [\"Employment gaps\", \"Tech mismatch (Tableau vs Power BI)\", \"Language mismatch\"],\n  \"source\": {\n    \"job_description_id\": \"jd_123\",\n    \"resume_id\": \"cv_456\",\n    \"cover_letter_id\": \"cl_789\"\n  }\n}\n\n\nEVALUATION PROCEDURE\n1) From the JD, list must-have qualifications, responsibilities, and named tools. Separate MUST vs NICE-TO-HAVE.\n2) Scan resume/CL for exact matches; then close synonyms/stems (be conservative). Count occurrences and note recency (latest roles first).\n3) Identify missing or weakly evidenced items. Penalize vague bullets and unquantified claims.\n4) Check ATS basics: single-column, standard section headings, contact info, consistent date formats, no images/icons/text boxes.\n5) Assess CL tailoring: opening hook tied to role/company, concrete evidence mapped to JD, concise close with call to action.\n6) Assign each metric 0\u201310 using anchors; compute weighted overall score. Provide pointed, high-impact fixes and example rewrites **only using existing facts** (if data is missing, instruct what to add\u2014not fabricate).\n\nCONSTRAINTS\n- JSON output only, exactly as specified.\n- Quote just enough text to justify (\u226425 words per quote).\n- If cover_letter_text is missing, still score the CL metric but explain the penalty.\n"
        }
      },
      "type": "@n8n/n8n-nodes-langchain.agent",
      "typeVersion": 2.2,
      "position": [
        144,
        0
      ],
      "id": "a1c5eff0-d77b-4cbb-83e6-8335a9dbe259",
      "name": "AI Agent",
      "executeOnce": false,
      "retryOnFail": true,
      "maxTries": 5
    },
    {
      "parameters": {
        "model": "moonshotai/kimi-k2-instruct-0905",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatGroq",
      "typeVersion": 1,
      "position": [
        112,
        224
      ],
      "id": "31adad02-2a5a-4ffe-8e92-93de42651671",
      "name": "Groq Chat Model",
      "credentials": {
        "groqApi": {
          "name": "Groq account"
        }
      }
    },
    {
      "parameters": {
        "jsonSchemaExample": "{\n  \"summary\": \"Brief overview of fit and biggest blockers.\",\n  \"overall\": {\n    \"weighted_score\": 0,\n    \"weights\": {\n      \"role_fit\": 0.30,\n      \"skills_keywords\": 0.25,\n      \"impact_evidence\": 0.20,\n      \"ats_compliance\": 0.15,\n      \"cover_letter\": 0.10\n    },\n    \"rationale\": \"One-liner explaining the overall score.\"\n  },\n  \"scores\": {\n    \"role_fit\": {\n      \"score\": 0,\n      \"evidence_found\": [\"Quoted line from resume/CL\", \"Quoted line from JD\"],\n      \"gaps\": [\"Missing must-have X\", \"No domain experience Y\"],\n      \"fixes\": [\"Add project showing X with metrics\", \"Clarify years doing Y\"]\n    },\n    \"skills_keywords\": {\n      \"score\": 0,\n      \"present\": [\"Exact term A\", \"Exact term B\"],\n      \"missing\": [\"Exact term C\", \"Exact term D\"],\n      \"fixes\": [\"Add C in Skills if true\", \"Name tool D in Project Z bullet\"]\n    },\n    \"impact_evidence\": {\n      \"score\": 0,\n      \"issues\": [\"Vague verbs\", \"No metrics\", \"Unclear scope\"],\n      \"example_rewrites\": [\n        \"Replace: \\\"Worked on reporting\\\" -> \\\"Built Power BI suite cutting refresh time 6h->45m (-87%)\\\"\"\n      ]\n    },\n    \"ats_compliance\": {\n      \"score\": 0,\n      \"problems\": [\"Two-column layout\", \"Icons in headers\", \"Missing dates\"],\n      \"fixes\": [\"Use single-column\", \"Standard section names\", \"Add MM/YYYY ranges\"]\n    },\n    \"cover_letter\": {\n      \"score\": 0,\n      \"critique\": [\"Generic opener\", \"Repeats resume\", \"No tie to company\"],\n      \"fixes\": [\"Open with JD-linked value\", \"Cite 2 quantified wins\", \"Keep \u2264300 words\"]\n    }\n  },\n  \"missing_keywords\": {\n    \"must_have\": [\"Term from JD essential 1\", \"Term essential 2\"],\n    \"should_have\": [\"Important term 1\"],\n    \"nice_to_have\": [\"Optional term 1\"]\n  },\n  \"top_fixes\": [\n    {\n      \"area\": \"Resume \u2192 Experience\",\n      \"action\": \"Quantify outcomes for Project A\",\n      \"why\": \"JD requires measurable impact\",\n      \"example\": \"Automated pipeline reducing failures 28% QoQ\"\n    },\n    {\n      \"area\": \"Resume \u2192 Skills\",\n      \"action\": \"List exact JD tools actually used\",\n      \"why\": \"Improves ATS matching\",\n      \"example\": \"Skills: Snowflake, dbt, Power BI, Celonis, Python\"\n    },\n    {\n      \"area\": \"Cover Letter\",\n      \"action\": \"Tie achievements to JD responsibilities\",\n      \"why\": \"Signals tailoring\",\n      \"example\": \"Shipped prompt-eval harness raising success 62%\u219279%\"\n    }\n  ],\n  \"keyword_occurrences\": [\n    {\n      \"term\": \"Power BI\",\n      \"in_resume\": 2,\n      \"in_cover_letter\": 0,\n      \"in_job_description\": 5\n    }\n  ],\n  \"risk_flags\": [\"Employment gaps\", \"Tech mismatch (Tableau vs Power BI)\", \"Language mismatch\"],\n  \"source\": {\n    \"job_description_id\": \"jd_123\",\n    \"resume_id\": \"cv_456\",\n    \"cover_letter_id\": \"cl_789\"\n  }\n}\n"
      },
      "type": "@n8n/n8n-nodes-langchain.outputParserStructured",
      "typeVersion": 1.3,
      "position": [
        288,
        224
      ],
      "id": "4de7d23c-6832-45f0-9cc2-85aa3847c889",
      "name": "Structured Output Parser"
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $json }}",
        "options": {}
      },
      "id": "9ab36a5b-d1a0-4965-a764-fd73d4c29a4d",
      "name": "Respond to Webhook",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.1,
      "position": [
        608,
        0
      ]
    },
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "evaluate-ats",
        "responseMode": "responseNode",
        "options": {}
      },
      "id": "a5f3dacb-e5d5-4af8-8f86-5b788cfb9f34",
      "name": "Webhook",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2.1,
      "position": [
        -112,
        0
      ],
      "webhookId": "0f93e18c-1bd2-4b15-8401-841c4ab4ed83"
    }
  ],
  "pinData": {
    "Webhook": [
      {
        "json": {
          "headers": {
            "host": "atomic-dayton-squad-printable.trycloudflare.com",
            "user-agent": "node",
            "content-length": "12617",
            "accept": "*/*",
            "accept-encoding": "gzip",
            "accept-language": "*",
            "cdn-loop": "cloudflare; loops=1; subreqs=1",
            "cf-connecting-ip": "2001:a61:5e8:b201:89b:67bd:e0a5:e487",
            "cf-ew-via": "15",
            "cf-ipcountry": "DE",
            "cf-ray": "9997d10631239276-MUC",
            "cf-visitor": "{\"scheme\":\"https\"}",
            "cf-warp-tag-id": "8fe37b18-359e-4dde-ad62-f896566a654b",
            "cf-worker": "trycloudflare.com",
            "connection": "keep-alive",
            "content-type": "application/json",
            "sec-fetch-mode": "cors",
            "x-forwarded-for": "2001:a61:5e8:b201:89b:67bd:e0a5:e487",
            "x-forwarded-proto": "https"
          },
          "params": {},
          "query": {},
          "body": {
            "resume_text": "Himanshu Ninawe  \n\ud83d\udcde +49-17660379682, \ud83d\udccc Munich, Germany, \ud83d\udce7 Ninawe.himanshu.24@gmail.com,\u26a1 Himanshu Ninawe  \n\nWork Experience  \n\nData Engineer & Operation Analyst, at Knorr-Bremse SfS GmbH, Munich, Germany. (Jan 24 - Present)  \n\u25cf Data Integration & Visualization: Performed ETL on data from Snowflake and SAP BW, creating insightful Power BI dashboards for operations KPIs, which enhanced decision-making processes.  \n\u25cf Advanced Analytics: Conducted descriptive, root cause, sensitivity, impact, and predictive analyses to improve process efficiency and operational performance.  \n\u25cf Machine Learning Development: Developed predictive models using Python, enabling initiative-taking strategies, and improving forecasting accuracy.  \n\u25cf Process Mining Optimization: Applied process mining techniques with Celonis to streamline operational workflows, reducing inefficiencies and increasing throughput.  \n\u25cf Global Business Process Leadership: As the Global BPR for operations across all Rail plants, led monthly KPI reviews, driving performance improvements and contributing to the achievement of yearly targets.  \n\nMaster\u2019s Thesis Student, at Ravensburg-Weingarten University, Weingarten, Germany. (March 24 \u2013 August 24)  \n\u201cIntegration of Generative AI in Smart Assistants\u201d  \n\u25cf Open-Source Smart Assistant Development: Created an open-source home assistant with Generative AI capabilities to overcome limitations like natural, human\u2011like conversations, ground\u2011up multilingual support, and offline functionality for non\u2011real\u2011time information.  \n\u25cf Comprehensive LLM Experimentation: Conducted extensive experiments with various methods including supervised fine\u2011tuning with Q\u2011LoRa, LLM function calling, Retrieval Augmented Generation (RAG) and Agentic workflows.  \n\u25cf Custom Dataset Generation: Generated a high\u2011quality Alpaca\u2011format dataset of approximately 1,000 entries using Llama 3 70B and Nematron 405B reward models; manually reviewed responses to enhance system accuracy.  \n\u25cf Edge Device Integration & Communication: Enabled task execution on an ESP32 connected to devices like lights and switches; established MQTT communication between the Raspberry Pi and ESP32, allowing offline operation without internet dependency.  \n\u25cf System performance Enhancement: Achieved a 95% accuracy, 90% precision, and 92% flexibility. Validated these results on a custom validation dataset featuring multilingual prompts, different moods, tones, and grammatical structures.  \n\nWorking Student, for MTU Solutions GmbH, Weingarten, Germany. (May 23 - October 23)  \n\u25cf Data Generation: Generated and processed 1.5\u202fmillion rows of synthetic data from a digital twin, leveraging automated scripts to ensure high\u2011volume, diverse datasets.  \n\u25cf Data Cleaning & Feature Engineering: Conducted data cleaning and feature engineering, increasing dataset variability by 20% through advanced techniques like Latin hypercube sampling.  \n\u25cf Predictive Modeling: Developed and optimized machine learning models (Random Forest, MLP, XGBoost, LSTM), improving fault localization accuracy, with ensemble methods performing best.  \n\u25cf Sensitivity Analysis & Feature Importance: Performed global and local sensitivity analyses and feature importance evaluation using methods like mRMR to identify key parameters influencing engine performance.  \n\u25cf Performance Improvement: Enhanced fault detection and reduced root cause analysis time, improving overall engine maintenance efficiency and downtime reduction.  \n\nProjects  \n\nMultimodal Information Retrieval System  \n\u25cf Multimodal Embeddings: For context involving images, text, and videos, used Nomic vision embedding models for visual data and Nomic text embedding models for textual data and ChromaDB to store the embeddings.  \n\u25cf Vision\u2011Language Model Integration: Integrated Moondream\u202f2, a Phi2\u2011based Vision Language Model (VLM), to enhance the processing and understanding of multimodal data.  \n\u25cf Enhanced Retrieval Capabilities: Improved retrieval and generation capabilities by combining multiple modalities, leading to more comprehensive and accurate outputs.  \n\nDigital Twin Fault Diagnostics Using LLMs  \n\u25cf Synthetic Data Generation and Transformation: Generated synthetic data from a digital twin of a single\u2011cylinder engine, performed EDA, cleaned the data, and transformed it into a DPO dataset.  \n\u25cf Model Fine\u2011Tuning: Fine\u2011tuned the Phi\u202f2 model to function as a digital twin, inputting real\u2011time data from six sensors and outputting 132 parameter sensor values to identify fault locations.  \n\u25cf LLM\u2011Based Fault Diagnostics: Utilized the fine\u2011tuned LLM as a predictive model for fault diagnostics, enhancing the ability to detect and locate engine faults effectively.  \n\nImplementing Long\u2011Term Memory in Language Models  \n\u25cf Conditional RAG Implementation: Employed Conditional Retrieval Augmented Generation to enable long\u2011term memory in LLMs by storing entire conversations in text files, which are recalled when a prompt ends with double question marks \"??\".  \n\u25cf Enhanced Recall with Graph RAG: Used Graph RAG to improve recall accuracy and precision, allowing the model to retrieve relevant past conversation data more effectively.  \n\u25cf Mixture of Experts (MoE) Model Integration: Integrated Phixtral, a Mixture of Experts model adept at \"needle in a haystack\" tasks, to enhance the model's performance in recalling specific information.  \n\nEducation  \n\n\u25cf MSc in Mechatronics (MINT), Ravensburg\u2011Weingarten University, Germany. GPA (2.1/5.0)  \n\u25cf B.E in Mechanical Engineering, Rashtrasant Tukadoji Maharaj Nagpur University, India. GPA (2.1/5.0)  \n\nSpecializations  \n\n\u25cf IBM Machine Learning Professional Certificate.  \n\u25cf IBM AI Engineering Professional Certificate.  \n\nSkills  \n\n\u25cf Programming & Scripting: Python, Linux, SQL, Prompting.  \n\u25cf Data Analysis and Visualization: Snowflake, NumPy and Pandas, Power BI, Excel, Qlik sense, SAP, Celonis.  \n\u25cf ML Frameworks: TensorFlow, PyTorch, Scikit\u2011learn, XGBoost, Lang\u2011chain, Ollama, Crew AI, Autogen, Graph RAG, Transformers, Diffusers.  \n\u25cf Orchestration and Deployment: GitHub, MLflow, Docker, Rest API.  \n\u25cf Database Management: RDBMS, NoSQL, Vector DB  \n\u25cf Technical Skills: Data Wrangling, Data Modeling, Data Visualization, Process Mining, Machine Learning, Fine\u2011tuning, Agentic Workflows, Synthetic Data generation.  \n\u25cf Soft Skills: Critical Thinking, Problem\u2011Solving, Excellent Communication, Team Collaboration, Leadership, Business Acumen.  \n\nLanguages  \n\nEnglish (C1), German (B1)",
            "cover_letter_text": "Himanshu Ninawe\nWildenfelser Stra\u00dfe 15\n81249 M\u00fcnchen, Germany\n\n22 October 2025\n\nHiring Manager\nSynthflow AI\nBerlin, Germany\n\nDear Hiring Manager,\n\nI am applying for the Applied LLM Engineer role at Synthflow AI, which I discovered on your careers page.\n\nI build production-grade LLM systems\u2014prompted, retrieval-aware, and observable\u2014using clean Python and rigorous evaluation so improvements reach users, not just notebooks.\n\nSynthflow\u2019s mission and scale resonate: 45M+ calls handled with 99.9% uptime, 5M+ hours saved, and 1,000+ customers. Your no-code platform and tight customer feedback loops match my focus on shipping dependable agents that measurably raise task completion and first-pass accuracy.\n\nRelevance of my skills: At Knorr-Bremse (Munich), I ship Python services and analytics to production (Snowflake/SAP BW \u2192 KPI platforms) and lead data/ML work informing monthly global reviews. In my M.Sc. thesis, I fine-tuned LLMs with parameter-efficient methods and built an evaluation harness, achieving 95% accuracy and 90% precision on a custom multilingual set. My projects include RAG pipelines (ChromaDB, graph-style retrieval, reranking) and agentic workflows, plus ML lifecycle tooling (Git, MLflow, Docker). Earlier at MTU Solutions, I generated/engineered 1.5M synthetic rows, boosting dataset variability by ~20% and improving fault-localization performance.\n\nHow I\u2019ll add value: I can design and iterate system/tool prompts tuned to call outcomes (intent/slot extraction, refusal policy, redaction), build a prompt-authoring co-pilot that lints risks, proposes tool schemas, and auto-generates eval cases from conversation logs, stand up config-as-code with versioned prompt bundles, rollout/rollback, and guardrails (pydantic validation, retrieval-aware prompting, safe fallbacks), and create offline/online evals (rubrics, regressions, canaries/A-B) wired into dashboards tracking accuracy, latency, cost, and robustness. I\u2019m disciplined about data hygiene, PII handling, and regional boundaries.\n\nIn sum, I bring hands-on prompt/RAG work, a production mindset, and an evaluation-first approach aligned with Synthflow\u2019s applied focus. I\u2019d welcome the opportunity to discuss how I can contribute to your customer co-pilot and the reasoning layer at scale.\n\nThank you for your time and consideration.\n\nSincerely,\nHimanshu Ninawe",
            "job_description": "Job Title: Applied LLM Engineer - Prompts, Evals & Agents\nCompany: G2i Inc.\nCompany Website: Not stated.\nLocation: Global Remote\nWork Model: Remote\nSeniority: Not stated.\nEmployment Type: Full time\nDepartment/Team: Engineering\nPosted: Not stated.\nApply by: Not stated.\nCompensation: Not stated.\nVisa/Relocation/Travel: Not stated.\n\nSummary:\nSynthflow AI is a no-code platform for deploying voice AI agents that automate phone calls across contact center operations and business process outsourcing (BPO) at scale. We help mid-market and enterprise companies manage routine calls to save teams time and resources.\n\nRequirements:\n- Python: 3+ years writing clean, tested, production code (typing, pytest, profiling); experience building small services/APIs (FastAPI preferred).\n- Prompt Engineering: Hands-on experience designing system/tool prompts, meta-prompting, rubric graders, and iterative prompt tuning based on real user data.\n- LLM Integration: Comfortable with major APIs (OpenAI/Anthropic/Google/Mistral), function/tool calling, streaming, and robust output handling.\n- Evaluation Mindset: Ability to define measurable success, create labeled datasets, and run methodical experiments/A/B tests.\n- Product Sense: Comfortable talking with customers, turning qualitative feedback into shipped improvements.\n- Data Hygiene: Practical experience cleaning, labeling, and balancing datasets; awareness of privacy/PII constraints.\n\nResponsibilities:\n- Design & iterate prompts (system, tool/function-calling, task prompts) to boost voice AI agent success, reliability, and tone.\n- Build co-pilots for customers to author their own prompts: meta-prompted assistants that suggest structures, lint for risks, autocomplete tool schemas, critique drafts, and generate eval cases.\n- Work directly with customer feedback and conversation logs to identify failure modes; translate them into prompt changes, guardrails, and data improvements.\n- Build eval datasets (success labels, rubrics, edge cases, regressions) and run offline/online evaluations (A/B tests, canaries) to quantify impact.\n- Create Python utilities/services for prompt versioning, config-as-code, rollout/rollback, and guardrails (policies, refusals, redaction).\n- Partner with PM/Success to define success metrics (task completion, first-pass accuracy, cost, latency) and instrument dashboards/alerts.\n- Own LLM integration details: function/tool schemas, output parsing/validation (pydantic), retrieval-aware prompting, and fallback strategies.\n- Ensure privacy & compliance (PII handling, anonymization, regional data boundaries) in datasets and logs.\n- Share learnings via concise docs, playbooks, and internal demos.\n\nTech Stack:\n- Python\n- FastAPI\n- OpenAI\n- Anthropic\n- Google\n- Mistral\n- pydantic\n- DSpy\n- MCP\n- LangGraph\n- LlamaIndex\n- Rasa\n- Langfuse\n- LangSmith\n- Braintrust\n- Qdrant\n- Weaviate\n- Pinecone\n- SQL\n- Grafana\n- OTel\n\nBenefits:\n- Own the reasoning layer and the customer co-pilot experience used at scale.\n- Ship fast in a tight customer feedback loop and see your impact measured in days, not quarters.\n\nApplication:\n- How to apply: Not stated.\n- Apply link/email: https://jobs.ashbyhq.com/synthflow/d5bba02f-1708-4368-aba1-1afdf695af40?utm_source=3bVopaRB11\n\nReference/Job ID: d5bba02f-1708-4368-aba1-1afdf695af40\n\nNotes:\nNot stated.\n\nSource: https://jobs.ashbyhq.com/synthflow/d5bba02f-1708-4368-aba1-1afdf695af40?utm_source=3bVopaRB11"
          },
          "webhookUrl": "https://atomic-dayton-squad-printable.trycloudflare.com/webhook/evaluate-ats",
          "executionMode": "production"
        }
      }
    ]
  },
  "connections": {
    "Groq Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "AI Agent",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Structured Output Parser": {
      "ai_outputParser": [
        [
          {
            "node": "AI Agent",
            "type": "ai_outputParser",
            "index": 0
          }
        ]
      ]
    },
    "AI Agent": {
      "main": [
        [
          {
            "node": "Respond to Webhook",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Webhook": {
      "main": [
        [
          {
            "node": "AI Agent",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "1b2931d7-8ca1-48ab-9a45-64eec904f9ae",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "91619af48a111996d57e467ff2315ebcea6bea0614914ccbffc4272c1511d582"
  },
  "id": "pyGw8Xkfa9DkwfLI",
  "tags": []
}